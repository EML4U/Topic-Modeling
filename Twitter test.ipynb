{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/twitter/election_dataset.pickle', 'rb') as handle:\n",
    "    twitter = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden, trump = twitter['biden'], twitter['trump']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden = [x for x in biden if 'trump' not in x[1].lower()]\n",
    "trump = [x for x in trump if 'biden' not in x[1].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from embedding import BertHuggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = BertHuggingface(8, model_name='bert-base-multilingual-cased', batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(data):\n",
    "    times, tweets = zip(*data)\n",
    "    embs = bert.embed(tweets)\n",
    "    z = zip(times, embs)\n",
    "    return list(z)\n",
    "\n",
    "if os.path.exists('data/twitter/biden_embeddings.pickle'):\n",
    "    with open('data/twitter/biden_embeddings.pickle', 'rb') as handle:\n",
    "        embs_biden = pickle.load(handle)\n",
    "else:\n",
    "    embs_biden = embed(biden)\n",
    "    with open('data/twitter/biden_embeddings.pickle', 'wb') as handle:\n",
    "        pickle.dump(embs_biden, handle)\n",
    "print('biden embedding done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "def normalized(a, axis=-1, order=2):\n",
    "    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n",
    "    l2[l2==0] = 1\n",
    "    return a / np.expand_dims(l2, axis)\n",
    "\n",
    "def create_moving_average(dataset, timeframe='hours'):\n",
    "    timefactor = 1 if timeframe == 'hours' else 24\n",
    "    moving_average = []\n",
    "    min_date = min([x[0] for x in dataset])\n",
    "    max_date = max([x[0] for x in dataset])\n",
    "    for each in range((max_date - min_date).days*(24 if timeframe == 'hours' else 1)):\n",
    "        d = min_date + datetime.timedelta(hours=each*timefactor)\n",
    "        points = [normalized(x[1]) for x in dataset if x[0] > d and x[0] < d + datetime.timedelta(hours=timefactor)]\n",
    "        if len(points):\n",
    "            moving_average.append(normalized(sum(points))) \n",
    "    return moving_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "def compute_cosine_similarities(X):\n",
    "    cosine_similarities = []\n",
    "    for i in range(len(X)):\n",
    "        cosine_similarity = sklearn.metrics.pairwise.cosine_similarity(X[i], X[0])\n",
    "        cosine_similarities.append(cosine_similarity.item())\n",
    "        \n",
    "    return cosine_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_ma_hours = create_moving_average(embs_biden)\n",
    "biden_cos_hours = compute_cosine_similarities(biden_ma_hours)\n",
    "plt.plot(biden_cos_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biden_ma_days = create_moving_average(embs_biden, timeframe='days')\n",
    "biden_cos_days = compute_cosine_similarities(biden_ma_days)\n",
    "plt.plot(biden_cos_days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/twitter/trump_embeddings.pickle'):\n",
    "    with open('data/twitter/trump_embeddings.pickle', 'rb') as handle:\n",
    "        embs_trump = pickle.load(handle)\n",
    "else:\n",
    "    embs_trump = embed(trump)\n",
    "    with open('data/twitter/trump_embeddings.pickle', 'wb') as handle:\n",
    "        pickle.dump(embs_trump, handle)\n",
    "print('trump embedding done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_ma_hours = create_moving_average(embs_trump)\n",
    "trump_cos_hours = compute_cosine_similarities(trump_ma_hours)\n",
    "plt.plot(trump_cos_hours)\n",
    "# small dips are always somewhat around 5-8am "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_ma_days = create_moving_average(embs_trump, timeframe='days')\n",
    "trump_cos_days = compute_cosine_similarities(trump_ma_days)\n",
    "plt.plot(trump_cos_days, marker='x')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trump_cos_hours.index([x for x in trump_cos_hours if x < 0.975][0]))\n",
    "print(trump_cos_hours.index([x for x in trump_cos_hours if x < 0.94][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min([x[0] for x in embs_trump]) + datetime.timedelta(hours=465))\n",
    "print(min([x[0] for x in embs_trump]) + datetime.timedelta(hours=480))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_max_tweets(data, amount=5):\n",
    "    data_r = data[1:]\n",
    "    maxes = []\n",
    "    for i in range(amount):\n",
    "        maxes.append(max(data_r))\n",
    "        data_r.remove(maxes[-1])\n",
    "    return [data.index(x) for x in maxes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine differences for all points to mean at 100 and plot their histogram\n",
    "d = min([x[0] for x in embs_trump]) + datetime.timedelta(hours=100)\n",
    "points = [normalized(x[1]) for x in embs_trump if x[0] > d and x[0] < d + datetime.timedelta(hours=1)]\n",
    "combi_100 = [trump_ma_hours[100]]\n",
    "combi_100.extend(points)\n",
    "trump_100 = compute_cosine_similarities(combi_100)\n",
    "plt.hist(trump_100[1:])\n",
    "\n",
    "five_max = find_max_tweets(trump_100, amount=5)\n",
    "for i in range(5):\n",
    "    print([x for x in trump if x[0] > d and x[0] < d + datetime.timedelta(hours=1)][five_max[i]][1], trump_100[five_max[i]],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine differences for all points to mean at 480 and plot their histogram\n",
    "d = min([x[0] for x in embs_trump]) + datetime.timedelta(hours=480)\n",
    "points = [normalized(x[1]) for x in embs_trump if x[0] > d and x[0] < d + datetime.timedelta(hours=1)]\n",
    "combi_480 = [trump_ma_hours[480]]\n",
    "combi_480.extend(points)\n",
    "trump_480 = compute_cosine_similarities(combi_480)\n",
    "plt.hist(trump_480[1:])\n",
    "\n",
    "five_max = find_max_tweets(trump_480, amount=5)\n",
    "for i in range(5):\n",
    "    print([x for x in trump if x[0] > d and x[0] < d + datetime.timedelta(hours=1)][five_max[i]][1], trump_480[five_max[i]],'\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### same with biden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine differences for all points to mean at 100 and plot their histogram\n",
    "d = min([x[0] for x in embs_biden]) + datetime.timedelta(hours=100)\n",
    "points = [normalized(x[1]) for x in embs_biden if x[0] > d and x[0] < d + datetime.timedelta(hours=1)]\n",
    "combi_100 = [biden_ma_hours[100]]\n",
    "combi_100.extend(points)\n",
    "biden_100 = compute_cosine_similarities(combi_100)\n",
    "plt.hist(biden_100[1:])\n",
    "five_max = find_max_tweets(biden_100, amount=5)\n",
    "for i in range(5):\n",
    "    print([x for x in biden if x[0] > d and x[0] < d + datetime.timedelta(hours=1)][five_max[i]][1], biden_100[five_max[i]],'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine differences for all points to mean at 480 and plot their histogram\n",
    "d = min([x[0] for x in embs_biden]) + datetime.timedelta(hours=480)\n",
    "points = [normalized(x[1]) for x in embs_biden if x[0] > d and x[0] < d + datetime.timedelta(hours=1)]\n",
    "combi_480 = [biden_ma_hours[480]]\n",
    "combi_480.extend(points)\n",
    "biden_480 = compute_cosine_similarities(combi_480)\n",
    "plt.hist(biden_480[1:])\n",
    "five_max = find_max_tweets(biden_480, amount=5)\n",
    "for i in range(5):\n",
    "    print([x for x in biden if x[0] > d and x[0] < d + datetime.timedelta(hours=1)][five_max[i]][1], biden_480[five_max[i]], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### some sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def load_sent_dataset(split = \"train\"):\n",
    "    ds = tfds.load('sentiment140', split=split, shuffle_files=False)\n",
    "    return ds\n",
    "\n",
    "def train_model(X,y,name= \"Sent_net\"):\n",
    "        \n",
    "        print(\"training model\", name)\n",
    "        X, X_test, y, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        loadModel = os.path.exists(name)\n",
    "        if not loadModel:\n",
    "            model = keras.Sequential()\n",
    "            model.add(keras.Input(shape=(X[0].shape)))\n",
    "            model.add(layers.Dense(100, activation=\"relu\"))\n",
    "            model.add(layers.Dense(50, activation=\"relu\"))\n",
    "            model.add(layers.Dense(1, activation = \"sigmoid\"))\n",
    "            model.compile(optimizer=\"rmsprop\",loss='mse')\n",
    "            history = model.fit(X,y, epochs = 5, validation_data = (X_test,y_test), batch_size = 64,verbose = 3)\n",
    "        \n",
    "            model.save(name)\n",
    "        else:\n",
    "            model = keras.models.load_model(name)\n",
    "        \n",
    "        return model\n",
    "    \n",
    "def preprocess_dataset(ds, encoder):\n",
    "    X = []\n",
    "    X_text= []\n",
    "    y= []\n",
    "    if not os.path.exists('data/embeded_twitter_ds.pkl'):\n",
    "        print(\"preprocessing dataset\")\n",
    "        for dicto in ds.take(100000):\n",
    "            X_text.append(str(dicto[\"text\"]))\n",
    "            y.append([float(dicto[\"polarity\"])/4.0])\n",
    "        print(True if [x for x in X_text if type(x)!=type('bla')] else False)\n",
    "        X = encoder.embed_bert(X_text)\n",
    "        X, y = np.asarray(X), np.asarray(y)\n",
    "        file = open('data/embeded_twitter_ds.pkl', 'wb')\n",
    "        pickle.dump([X, y, X_text], file)\n",
    "    else:\n",
    "        file = open('data/embeded_twitter_ds.pkl', 'rb')\n",
    "        X, y, X_text = pickle.load(file)     \n",
    "    file.close()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('Sent_net_trained'):\n",
    "    ds = load_sent_dataset()\n",
    "    X, Y = preprocess_dataset(ds, embedder)\n",
    "    with tf.device('/GPU:0'):\n",
    "        sentiment_model = train_model(X, Y)\n",
    "    sentiment_model.save('Sent_net_trained')\n",
    "else:\n",
    "    sentiment_model = keras.models.load_model('Sent_net_trained')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(data, model):\n",
    "    times, tweets = zip(*data)\n",
    "    embs = model.predict(tweets)\n",
    "    z = zip(times, embs)\n",
    "    return list(z)\n",
    "\n",
    "\n",
    "    emb_list = []\n",
    "    num_steps = int(math.ceil(len(text_list) / self.batch_size))\n",
    "    print('Splitting into', num_steps, 'batches...')\n",
    "    for i in range(num_steps):\n",
    "        ul = min((i + 1) * self.batch_size, len(text_list))\n",
    "        subset = text_list[i * self.batch_size:ul]\n",
    "        me = self.bert.encode(subset)\n",
    "        emb_list.append(me)\n",
    "    embeddings = np.vstack(emb_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.experimental.list_physical_devices('GPU')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/twitter/twitter_sentiments.pickle', 'wb') as handle:\n",
    "    pickle.dump((biden_sentiment, trump_sentiment), handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/twitter_sentiments.pickle'):\n",
    "    with open('data/twitter/twitter_sentiments.pickle', 'rb') as handle:\n",
    "        biden_sentiment, trump_sentiment = pickle.load(handle)\n",
    "else:\n",
    "    print('sentimenting biden...')\n",
    "    biden_sentiment = predict_sentiment(embs_biden, sentiment_model)\n",
    "    print('sentimenting trump...')\n",
    "    trump_sentiment = predict_sentiment(embs_trump, sentiment_model)\n",
    "    with open('data/twitter/twitter_sentiments.pickle', 'wb') as handle:\n",
    "        pickle.dump((biden_sentiment, trump_sentiment), handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_sentiment[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markup of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in trump if 'biden' in x[1].lower()])/len(trump)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([x for x in biden if 'trump' in x[1].lower()])/len(biden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both = [x for x in twitter['biden'] if 'trump' in x[1].lower()] + [x for x in twitter['trump'] if 'biden' in x[1].lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists('data/twitter/both_embeddings.pickle'):\n",
    "    with open('data/twitter/both_embeddings.pickle', 'rb') as handle:\n",
    "        embs_both = pickle.load(handle)\n",
    "else:\n",
    "    embs_both = embed(both)\n",
    "    with open('data/twitter/both_embeddings.pickle', 'wb') as handle:\n",
    "        pickle.dump(embs_both, handle)\n",
    "\n",
    "both_ma_hours = create_moving_average(embs_both)\n",
    "both_cos_hours = compute_cosine_similarities(both_ma_hours)\n",
    "plt.plot(both_cos_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "both_ma_days = create_moving_average(embs_both, timeframe='days')\n",
    "both_cos_days = compute_cosine_similarities(both_ma_days)\n",
    "plt.plot(both_cos_days, marker='x')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
